<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="/static/index1.css">
    <title>Document</title>
</head>
<body>
    <div class="header">
        <div class="text"><a style="color: white; text-decoration:none;" href="/">Voize</a></div>
        <div class="text1"><a style="color: white; text-decoration:none;" href="/about">About us</a></div>
    </div>
    <div style="margin-left: 60px; margin-right: 60px;  overflow: auto;" class="main">
        <h3 style="margin-top:30px;">The Creators:</h3>
        16010121006 - Alfiya Anware<br>
        16010121007 - Alekya Arra<br>
        16010121011 - Azhar Bamne<br><br>

        <h3>Problem Definition:</h3>
        To build a comprehensive Speech Emotion Recognizer (SER) that caters to various applications and provides a real-time emotional analysis of spoken language. Speech emotion recognition systems traditionally focus on limited datasets and basic emotion categories. The need for more accurate and nuanced emotion detection, especially in real-time applications, has prompted us to reevaluate and enhance existing systems. Our goal is to address these limitations by diversifying datasets, incorporating a broader range of emotions, and integrating speech-to-text capabilities.

        <br><br>
        <h3>Our Take:</h3>
        <ul style="list-style-position: inside;">
        <li>Diversify dataset: Include a wider variety of accents, languages, and age groups in the dataset to improve the model's adaptability to diverse user demographics.</li>
        <li>Incorporate more emotions - mixture of emotions: Expand emotion categories beyond basic ones (happy, sad, angry) to include nuanced emotions, capturing the complexity of human expression.</li>
        <li>Speech to Text: Integrate a robust Speech-to-Text (STT) feature to enhance the system's functionality. This addition will allow the recognizer to not only identify emotions but also provide a textual representation of the spoken content.</li>
        <li>Real-Time Recognizer App: Develop a real-time recognizer application that users can interact with, providing instant feedback on their emotional expressions. This application can find applications in various scenarios, from customer service to mental health support.</li>
        </ul>
        <br>

        <h3>Tech Stack:</h3>
        <ul style="list-style-position: inside;">
        <li>Python - tensorflow</li>
        Leverage Python as the primary programming language for its ease of use and extensive libraries, with options to use either PyTorch or TensorFlow as the deep learning framework.</li>
        
        <li>Flask/ Django:</li>
        Use Flask or Django as the web framework to build a scalable and efficient backend for the application.
        
        <li>Librosa library:</li>
        Employ the Librosa library to extract relevant audio features for emotion recognition, such as pitch, tempo, and spectral features.
        </ul>
        <br>

        <h3>Applications: </h3>
        <br>
        There are many applications of SER. Some of them include:
        <ol style="list-style-position: inside;">
        
        <li>AI understands us better:</li>
        Enable the system to understand users on a deeper emotional level, improving human-computer interaction and personalization.

        <li>Customer service:</li>
        Implement the SER in customer service applications to analyze customer emotions during calls, ensuring better service and issue resolution.

        <li>Help for autism spectrum:</li>
        Utilize the SER to assist individuals on the autism spectrum in understanding and responding to emotions, promoting social interaction and communication.

        <li>Educational applications:</li>
        Integrate the recognizer into educational tools to analyze students' engagement and emotional responses during online learning sessions.

        <li>Mental health support:</li>
        Provide real-time emotional analysis as part of mental health support applications, aiding therapists in assessing patients' emotional states during therapy sessions.

        <li>Support for hearing impairments:</li>
        Enhance accessibility by providing textual representations of spoken content through the integrated Speech-to-Text feature. This ensures that individuals with hearing impairments can also benefit from the emotional analysis and participate in conversations where traditional audio cues may be insufficient. The SER can serve as a valuable tool for facilitating communication and understanding emotions for the hearing-impaired community.

        <li>Mood-Based Music Recommendation System:</li>
        Enhance the user experience by integrating the SER into music recommendation systems. Analyzing real-time emotional cues, the system can dynamically adjust music playlists based on the user's mood, creating a personalized and emotionally resonant listening experience. This feature can be particularly beneficial for individuals with hearing impairments, offering them an inclusive and enjoyable music exploration platform.
        </ol>

        <br>


        <h3>References:</h3>

        https://docs.google.com/document/d/1Ask_IiEN4HNWeM9SWOpqRnqnk88X6yQylpP644fja7o/edit?usp=sharing<br>
        https://www.sciencedirect.com/science/article/pii/S1877050920318512<br>
        https://link.springer.com/article/10.1007/s10772-020-09672-4<br>
        https://www.sciencedirect.com/science/article/pii/S1877050923002272<br>
        https://www.sciencedirect.com/science/article/pii/S2667305323000911<br>

        <br><br><br>


    </div>
</body>
</html>